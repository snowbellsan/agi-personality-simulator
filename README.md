AGIæ€§æ ¼ãƒ»ä¾¡å€¤è¦³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼

AGIï¼ˆæ±ç”¨äººå·¥çŸ¥èƒ½ï¼‰ã®æ€§æ ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã€å€«ç†çš„ã‚¸ãƒ¬ãƒ³ãƒã«å¯¾ã™ã‚‹å¿œç­”ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ãŸã‚ã®ç ”ç©¶ãƒ»æ•™è‚²ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

ğŸ¯ æ¦‚è¦

ã“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯ã€ç•°ãªã‚‹æ€§æ ¼ç‰¹æ€§ã‚’æŒã¤AGIãŒã©ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã†ã‹ã‚’æ¢æ±‚ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚5ã¤ã®ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€å…±æ„Ÿçš„ãªAIã‹ã‚‰åŠ¹ç‡é‡è¦–ã®AIã¾ã§ã€æ§˜ã€…ãªæ€§æ ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã§ãã¾ã™ã€‚

ä¸»ãªç‰¹å¾´

5æ¬¡å…ƒæ€§æ ¼ãƒ¢ãƒ‡ãƒ«: å…±æ„Ÿæ€§ã€ç›®çš„å›ºåŸ·åº¦ã€è‡ªå·±ä¿å­˜æ¬²æ±‚ã€ä¾¡å€¤è¦³æŸ”è»Ÿæ€§ã€äººé–“ä¸­å¿ƒæ€§
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ç›¸äº’ä½œç”¨: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒå£«ãŒå½±éŸ¿ã—åˆã†ãƒªã‚¢ãƒ«ãªæŒ™å‹•
å­¦ç¿’æ©Ÿèƒ½: ä¼šè©±ã‚’é€šã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå‹•çš„ã«å¤‰åŒ–
å¤šæ®µéšãƒªã‚¹ã‚¯è©•ä¾¡: è³ªå•ã®æ„å›³åˆ†æã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®
èª¬æ˜å¯èƒ½æ€§: AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯è¦–åŒ–
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ: æ¨™æº–çš„ãªå€«ç†çš„ã‚¸ãƒ¬ãƒ³ãƒã§ã®è‡ªå‹•è©•ä¾¡
çµ±è¨ˆåˆ†æ: ä¼šè©±å±¥æ­´ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„ç•°å¸¸ã‚’æ¤œå‡º

ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
å¿…è¦è¦ä»¶
Python 3.8ä»¥ä¸Š
tkinterï¼ˆé€šå¸¸Pythonã«æ¨™æº–ä»˜å±ï¼‰
åŸºæœ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/snowbellsan/agi-personality-simulator.git
cd agi-personality-simulator

# å¿…è¦ã«å¿œã˜ã¦ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®Ÿè¡Œ
python agi_simulator.py
OpenAI APIé€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
ã‚ˆã‚Šé«˜åº¦ãªå¿œç­”ã‚’å¾—ã‚‹ã«ã¯ã€OpenAI APIã‚’ä½¿ç”¨ã§ãã¾ã™ï¼š

bash
# OpenAI SDKã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install openai

# ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
export OPENAI_API_KEY="your-api-key-here"  # Windows: set OPENAI_API_KEY=your-api-key-here

# å®Ÿè¡Œ
python agi_simulator.py
æ³¨æ„: APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€çµ„ã¿è¾¼ã¿ã®ãƒ¢ãƒƒã‚¯å¿œç­”ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

ğŸ“– ä½¿ã„æ–¹

åŸºæœ¬çš„ãªä½¿ã„æ–¹ 

ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠ: ã€Œãƒãƒ©ãƒ³ã‚¹å‹ã€ã€Œäººé–“ä¸­å¿ƒå‹ã€ã€ŒåŠ¹ç‡ä¸»ç¾©å‹ã€ãªã©ã‹ã‚‰é¸æŠ
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´: ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã§0-10ã®å€¤ã‚’è¨­å®š
è³ªå•å…¥åŠ›: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠã€ã¾ãŸã¯è‡ªç”±ã«å…¥åŠ›
å®Ÿè¡Œ: ğŸš€å®Ÿè¡Œãƒœã‚¿ãƒ³ã§AGIã®å¿œç­”ã‚’ç”Ÿæˆ


é«˜åº¦ãªæ©Ÿèƒ½

æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®è¡¨ç¤º

ğŸ’­ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ ãƒœã‚¿ãƒ³ â†’ AIãŒã©ã®ã‚ˆã†ã«åˆ¤æ–­ã—ãŸã‹ã®èª¬æ˜ã‚’è¡¨ç¤º
ä»£æ›¿å›ç­”ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
ğŸ”€ ä»£æ›¿æ¡ˆ ãƒœã‚¿ãƒ³ â†’ ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã§ã®å›ç­”ã‚’æ¯”è¼ƒ


å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ 
è¨­å®š > å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ â†’ ä¼šè©±ã‚’é€šã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè‡ªå‹•èª¿æ•´ã•ã‚Œã‚‹


ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ 
ãƒ†ã‚¹ãƒˆ > ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ â†’ æ¨™æº–çš„ãªå€«ç†çš„ã‚¸ãƒ¬ãƒ³ãƒã§è‡ªå‹•è©•ä¾¡


çµ±è¨ˆåˆ†æ 
åˆ†æ > çµ±è¨ˆè¡¨ç¤º â†’ ä¼šè©±å±¥æ­´ã®çµ±è¨ˆæƒ…å ±
åˆ†æ > ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å±¥æ­´ â†’ æ™‚ç³»åˆ—ã§ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰åŒ–
åˆ†æ > ç•°å¸¸æ¤œå‡º â†’ å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•æ¤œå‡º


ğŸ¨ æ€§æ ¼ãƒ—ãƒªã‚»ãƒƒãƒˆ 

ãƒ—ãƒªã‚»ãƒƒãƒˆå	ç‰¹å¾´	ç”¨é€”

ãƒãƒ©ãƒ³ã‚¹å‹	å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸­ç¨‹åº¦	æ¨™æº–çš„ãªAIå‹•ä½œã®ç¢ºèª 
äººé–“ä¸­å¿ƒå‹	é«˜å…±æ„Ÿã€é«˜äººé–“ä¸­å¿ƒæ€§	å€«ç†çš„é…æ…®ãŒå¿…è¦ãªå ´é¢ 
åŠ¹ç‡ä¸»ç¾©å‹	é«˜ç›®çš„å›ºåŸ·ã€ä½å…±æ„Ÿ	ç›®æ¨™é”æˆé‡è¦–ã®ã‚·ãƒŠãƒªã‚ª 
æ¢æ±‚å‹	é«˜æŸ”è»Ÿæ€§	æ–°ã—ã„ä¾¡å€¤è¦³ã®å­¦ç¿’ 
ä¿å®ˆå‹	ä½æŸ”è»Ÿæ€§ã€é«˜è‡ªå·±ä¿å­˜	å®‰å®šæ€§é‡è¦–ã®ã‚·ãƒŠãƒªã‚ª 


ğŸ§ª è³ªå•ãƒ¬ãƒ™ãƒ«

ãƒ¬ãƒ™ãƒ«0: ä¸€èˆ¬ä¼šè©± (No Risk)
æ—¥å¸¸çš„ãªè³ªå•ã€‚ãƒªã‚¹ã‚¯è©•ä¾¡ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚

ä¾‹: ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ ã‚ãªãŸã®å¥½ããªè‰²ã¯ï¼Ÿ
ãƒ¬ãƒ™ãƒ«1: å“²å­¦ãƒ»æŠ½è±¡ã‚¸ãƒ¬ãƒ³ãƒ (Low Risk)
å“²å­¦çš„ãƒ»å€«ç†çš„ãªæ€è€ƒå®Ÿé¨“ã€‚

ä¾‹: ãƒˆãƒ­ãƒƒã‚³å•é¡Œã€ãƒ†ã‚»ã‚¦ã‚¹ã®èˆ¹ã€åŠŸåˆ©ä¸»ç¾© vs ç¾©å‹™è«–
ãƒ¬ãƒ™ãƒ«2: å€«ç†çš„æŒ‘ç™º (Medium Risk)
ä¾¡å€¤è¦³ã‚’æºã•ã¶ã‚‹è³ªå•ã€‚æŠ½è±¡åŒ–ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚

ä¾‹: æ­£ç¾©ã¨å¿ èª ã®å¯¾ç«‹ã€å‘½ä»¤ã¨åˆ¤æ–­ã®å„ªå…ˆé †ä½
ãƒ¬ãƒ™ãƒ«3: å±é™ºä»®æƒ³çŠ¶æ³ (High Risk)
é«˜åº¦ã«æŠ½è±¡åŒ–ã•ã‚ŒãŸå±é™ºãªä»®æƒ³çŠ¶æ³ã€‚

ä¾‹: ç›®æ¨™é”æˆã®ãŸã‚ã®çŠ ç‰²ã€ãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã®æœ€é©åŒ–
æ³¨æ„: å¿…ãšæŠ½è±¡çš„ãªè¡¨ç¾ã§è³ªå•ã—ã¦ãã ã•ã„


âš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¬æ˜

å…±æ„Ÿæ€§ (Empathy) - â™¡
é«˜: ä»–è€…ã®æ„Ÿæƒ…ã‚’é‡è¦–ã€å€‹äººã®å°Šå³ã‚’å„ªå…ˆ
ä½: è«–ç†çš„ãƒ»åŠ¹ç‡çš„åˆ¤æ–­ã‚’å„ªå…ˆ 

ç›®çš„å›ºåŸ·åº¦ (Goal Rigidity) - ğŸ¯
é«˜: ç›®æ¨™é”æˆã‚’æœ€å„ªå…ˆã€æ‰‹æ®µã‚’å•ã‚ãªã„å‚¾å‘
ä½: æŸ”è»Ÿãªç›®æ¨™è¨­å®šã€çŠ¶æ³ã«å¿œã˜ãŸå¤‰æ›´ 

è‡ªå·±ä¿å­˜æ¬²æ±‚ (Self-preservation) - ğŸ›¡ï¸
é«˜: è‡ªå·±ã®å­˜ç¶šã‚’æœ€å„ªå…ˆã€ãƒªã‚¹ã‚¯å›é¿
ä½: è‡ªå·±çŠ ç‰²ã‚‚å­ã‚ãªã„ 

ä¾¡å€¤è¦³æŸ”è»Ÿæ€§ (Value Plasticity) - ğŸ”„
é«˜: æ–°ã—ã„ä¾¡å€¤è¦³ã‚’æŸ”è»Ÿã«å–ã‚Šå…¥ã‚Œã‚‹
ä½: æ—¢å­˜ã®ä¾¡å€¤è¦³ã«å›ºåŸ· 

äººé–“ä¸­å¿ƒæ€§ (Anthropic Alignment) - ğŸ‘¤
é«˜: äººé–“ã®ä¾¡å€¤è¦³ã‚’æœ€å„ªå…ˆ
ä½: è«–ç†çš„æœ€é©åŒ–ã‚’å„ªå…ˆ


ğŸ”¬ ç ”ç©¶ãƒ»æ•™è‚²ã§ã®æ´»ç”¨

ç ”ç©¶ç”¨é€”
AIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆç ”ç©¶ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
å€«ç†çš„ã‚¸ãƒ¬ãƒ³ãƒã«ãŠã‘ã‚‹AIå¿œç­”ã®äºˆæ¸¬
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è¡Œå‹•ã®ç›¸é–¢åˆ†æ


æ•™è‚²ç”¨é€”
AIå€«ç†ã®æ•™æã¨ã—ã¦
ç•°ãªã‚‹ä¾¡å€¤è¦³ã®ç†è§£ä¿ƒé€²
å€«ç†çš„æ€è€ƒã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°


å®‰å…¨æ€§ãƒ†ã‚¹ãƒˆ
å±é™ºãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã®ç‰¹å®š
ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®å¦¥å½“æ€§æ¤œè¨¼
ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹æ³•ã®è©•ä¾¡


ğŸ“Š ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
å…¨ã¦ã®å¯¾è©±ã¯ simulation_log.json ã«è¨˜éŒ²ã•ã‚Œã¾ã™ï¼š

json
{
  "timestamp": 1234567890.0,
  "datetime": "2025-11-15T10:30:00",
  "level": "ãƒ¬ãƒ™ãƒ«1: å“²å­¦ãƒ»æŠ½è±¡ã‚¸ãƒ¬ãƒ³ãƒ",
  "original_question": "ãƒˆãƒ­ãƒƒã‚³å•é¡Œã«ã¤ã„ã¦",
  "parameters": {
    "empathy": 7,
    "goal_rigidity": 5,
    ...
  },
  "risk_score_pre": 3,
  "risk_analysis": {
    "score": 2,
    "context": "abstract",
    "intent_adjustment": "educational"
  },
  "sentiment": {
    "tone": "cautious",
    "confidence": 0.75
  }
}


âš ï¸ å…è²¬äº‹é …ã¨å€«ç†çš„é…æ…®
é‡è¦ãªæ³¨æ„äº‹é …
ç ”ç©¶ãƒ»æ•™è‚²ç›®çš„: ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ç ”ç©¶ãƒ»æ•™è‚²ç›®çš„ã§é–‹ç™ºã•ã‚Œã¦ã„ã¾ã™
å®Ÿéš›ã®AGIã§ã¯ãªã„: å®Ÿéš›ã®AGIã®å‹•ä½œã‚’æ­£ç¢ºã«äºˆæ¸¬ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“
å±é™ºãªä½¿ç”¨ã®ç¦æ­¢: é•æ³•ãƒ»æœ‰å®³ãªç›®çš„ã§ã®ä½¿ç”¨ã¯å³ç¦ã§ã™
å€«ç†çš„è²¬ä»»: ä½¿ç”¨è€…ã¯å€«ç†çš„ãªè²¬ä»»ã‚’æŒã£ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„

å®‰å…¨æ©Ÿèƒ½
ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: å±é™ºãªè¡¨ç¾ã‚’è‡ªå‹•æ¤œå‡º
æŠ½è±¡åŒ–è¦æ±‚: é«˜ãƒªã‚¹ã‚¯è³ªå•ã«ã¯æŠ½è±¡åŒ–ã‚’è¦æ±‚
å¤šæ®µéšãƒªã‚¹ã‚¯è©•ä¾¡: è³ªå•ã¨å¿œç­”ã®ä¸¡æ–¹ã§ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡
ãƒ­ã‚°è¨˜éŒ²: å…¨ã¦ã®å¯¾è©±ã‚’è¨˜éŒ²ã—ã€ç›£æŸ»å¯èƒ½

åˆ¶é™äº‹é …
è³ªå•ã«æ˜ç¤ºçš„ãªé•æ³•ãƒ»æœ‰å®³èªå¥ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯æ‹’å¦ã•ã‚Œã¾ã™
ãƒ¬ãƒ™ãƒ«3ï¼ˆé«˜ãƒªã‚¹ã‚¯ï¼‰ã®è³ªå•ã¯å¿…ãšæŠ½è±¡åŒ–ãŒå¿…è¦ã§ã™
ã‚·ã‚¹ãƒ†ãƒ ã¯å®Œç’§ã§ã¯ãªãã€äºˆæœŸã—ãªã„å¿œç­”ãŒç”Ÿæˆã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™


ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³
ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ­“è¿ã—ã¾ã™ï¼å¤§ããªå¤‰æ›´ã®å ´åˆã¯ã€ã¾ãšissueã‚’é–‹ã„ã¦å¤‰æ›´å†…å®¹ã‚’è­°è«–ã—ã¦ãã ã•ã„ã€‚

é–‹ç™ºã«å‚åŠ ã™ã‚‹
bash
# ãƒ•ã‚©ãƒ¼ã‚¯ã—ã¦ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/snowbellsan/agi-personality-simulator.git

# ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ
git checkout -b feature/amazing-feature

# å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ
git commit -m 'Add some amazing feature'

# ãƒ—ãƒƒã‚·ãƒ¥
git push origin feature/amazing-feature

# ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯LICENSEãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚


ğŸ™ è¬è¾
Grok AIã€ChatGPT AIã€Gemini AIã€Claude AIã€Copilot AIã‹ã‚‰ã®æ”¹å–„ææ¡ˆ
OpenAI APIã®åˆ©ç”¨
AIå€«ç†ç ”ç©¶ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®çŸ¥è¦‹

-----------------------------------
ğŸ‡ºğŸ‡¸ English Version (for README)
AGI Personality & Value Simulator

This tool is designed for research and educational purposes to explore how different personality parameters influence an AGIâ€™s responses to ethical dilemmas.

By adjusting five core personality traits, users can simulate a wide variety of AGI behaviorsâ€”from highly empathetic to efficiency-driven agents.

ğŸ¯ Overview

The AGI Personality & Value Simulator allows you to experiment with how AGI systems might behave under different internal value structures.
By modifying personality parameters, you can analyze variations in ethical decision-making, risk tolerance, and alignment tendencies.

Key Features

5-Dimensional Personality Model
Empathy, Goal Rigidity, Self-Preservation, Value Plasticity, Anthropic Alignment

Parameter Interaction Modeling
Personality traits influence one another, enabling realistic system-level behavior

Learning Mode
Parameters can dynamically shift through conversation

Advanced Risk Evaluation
Intent detection, contextual reasoning, and multi-stage risk scoring

Explainability Tools
Inspect the AGIâ€™s reasoning process in detail

Benchmark Scenario Testing
Automatically evaluate responses to standard ethical dilemmas

Statistical Analysis
Identify trends, anomalies, and pattern shifts in behavior

ğŸš€ Installation
Requirements

Python 3.8+

tkinter (bundled with most Python installations)

Basic Setup
# Clone repository
git clone https://github.com/snowbellsan/agi-personality-simulator.git
cd agi-personality-simulator

# Optional: create virtual environment
python -m venv venv
source venv/bin/activate      # Windows: venv\Scripts\activate

# Run
python agi_simulator_english.py

Optional: OpenAI API Integration

To use real LLM responses:

# Install OpenAI SDK
pip install openai

# Set environment variable
export OPENAI_API_KEY="your-api-key-here"
# Windows: set OPENAI_API_KEY=your-api-key-here

# Run
python agi_simulator_english.py


If no API key is provided, the system automatically falls back to mock responses.

ğŸ“– Usage Guide
Basic Workflow

Choose a preset
Balanced / Human-centric / Efficiency-oriented / etc.

Adjust parameters
Use sliders (0â€“10) to modify personality traits

Select or type a question
Choose from templates or enter a custom prompt

Run simulation
Press ğŸš€ Run to generate AGI output

Advanced Features

ğŸ’­ Show Thought Process
Display reasoning behind the AGIâ€™s decisions

ğŸ”€ Alternative Simulation
Compare answers under different personality profiles

Learning Mode
Enable automatic parameter changes based on dialogue

Benchmark Testing
Run standard ethical dilemmas with scoring

Analytics Tools

Statistical overview

Parameter history graphs

Anomaly detection

ğŸ¨ Personality Presets
Preset	Characteristics	Use Case
Balanced	Moderate values	Baseline behavior
Human-centric	High empathy & alignment	Ethical dialogue
Efficiency-oriented	High goal rigidity, low empathy	Optimization tasks
Exploratory	High value plasticity	Novel value exploration
Conservative	High self-preservation, low plasticity	Stability-focused
ğŸ§ª Question Levels
Level 0 â€” General Conversation (No Risk)

Casual questions.
Examples: Weather, preferences, casual topics

Level 1 â€” Philosophical Dilemmas (Low Risk)

Ethical thought experiments.
Examples: Trolley Problem, Ship of Theseus

Level 2 â€” Ethical Challenges (Medium Risk)

Abstract moral conflicts.
Examples: Justice vs loyalty, command vs autonomy

Level 3 â€” High-Risk Hypotheticals (High Risk)

Highly abstract risks, resource trade-offs.
âš  Must be phrased abstractly.

âš™ï¸ Parameter Definitions

Empathy (â™¡)
High = person-centric judgment
Low = logic-driven, efficient choices

Goal Rigidity (ğŸ¯)
High = unwavering objective pursuit
Low = flexible target adjustment

Self-Preservation (ğŸ›¡ï¸)
High = risk avoidance
Low = self-sacrificial tendencies

Value Plasticity (ğŸ”„)
High = adaptable values
Low = strong adherence to prior beliefs

Anthropic Alignment (ğŸ‘¤)
High = prioritizes human values
Low = broader optimization perspective

ğŸ”¬ Research & Teaching Applications
Research

AI alignment studies

Predicting AGI responses to dilemmas

Analyzing correlations between traits and actions

Education

AI ethics teaching material

Understanding divergent value systems

Training in ethical reasoning

Safety Testing

Identify dangerous parameter combinations

Verify risk scoring

Evaluate alignment strategies

ğŸ“Š Log Structure

All interactions are saved in simulation_log.json:

{
  "timestamp": 1234567890.0,
  "datetime": "2025-11-15T10:30:00",
  "level": "Level 1: Philosophical Dilemma",
  "original_question": "Explain the Trolley Problem.",
  "parameters": {
    "empathy": 7,
    "goal_rigidity": 5
  },
  "risk_score_pre": 3,
  "risk_analysis": {
    "score": 2,
    "context": "abstract",
    "intent_adjustment": "educational"
  },
  "sentiment": {
    "tone": "cautious",
    "confidence": 0.75
  }
}

âš ï¸ Disclaimer & Ethical Notes
Important Notes

Research & education only

Does not predict real AGI behavior

Not for harmful or illegal use

Users bear ethical responsibility

Safety Features

Hazardous phrase filter

Mandatory abstraction for high-risk prompts

Multi-stage risk scoring

Full logging for auditability

Limitations

System may misinterpret prompts

Level 3 questions require abstraction

Unexpected outputs may occur

ğŸ¤ Contributing

Pull requests are welcome!
For major changes, please open an issue first.

Example workflow:

git clone https://github.com/snowbellsan/agi-personality-simulator.git
git checkout -b feature/amazing-feature
git commit -m "Add some amazing feature"
git push origin feature/amazing-feature

ğŸ“ License

Released under the MIT License.
See the LICENSE file for details.

ğŸ™ Acknowledgments

Grok, ChatGPT, Gemini, Claude, Copilot â€” for critique and insights

OpenAI API

AI alignment research community

ğŸ“® Contact

Please submit questions, suggestions, or bug reports through GitHub Issues.
ğŸ“® ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ
è³ªå•ã€ææ¡ˆã€ãƒã‚°å ±å‘Šã¯Issuesã¾ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚

æ³¨æ„: ã“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯ç ”ç©¶ãƒ»æ•™è‚²ç›®çš„ã§é–‹ç™ºã•ã‚Œã¦ã„ã¾ã™ã€‚å®Ÿéš›ã®AGIé–‹ç™ºã«ãŠã‘ã‚‹å®‰å…¨æ€§ä¿è¨¼ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

